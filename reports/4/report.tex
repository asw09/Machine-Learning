\documentclass[a4paper]{article}
\usepackage{fullpage, titling, amsmath, footnote, listings, graphicx, subfig, lscape}
\makesavenoteenv{tabular}
\setlength{\droptitle}{-50pt}

\title{Machine Learning Assignment 5: ANOVA Test}

\author{Group 31 \\ Andrew West}

\begin{document}
\maketitle

\section{General Results (Noisy Data)}

Results of testing the various algorithms on noisy data, when trained on clean data, were as follows:\\

\begin{center}
  \begin{tabular}{c|cccccc|c}
    Algorithm & Anger & Disgust & Fear & Happiness & Sadness & Surprise & Average $F_1$\\
    \hline
    Decision Trees 	& 0.333 & 0.919 & 0.727 & 0.950 & 0.667 & 0.952 & 0.7581 \\
	ANN 			& 0.875 & 0.971 & 0.914 & 1.000	& 0.889 & 1.000 & 0.9416 \\
	CBR System 		& 0.424 & 0.000 & 0.882 & 1.000	& 0.889 & 0.952 & 0.6913 \\
  \end{tabular}\\
  \end{center}
  (For these and all subsequent ANN results, the single 6-output net was used)\\
  
From these results, it is possible to say that Neural Networks appear to be the best learning algorithm to use. This can
been seen by comparison of the average $F_1$ measures for each algorithm. As these results were produced by training on
the entire clean data set and tested on the entire noisy set, the values for each measure for the ANN system and the
Case-Based Reasoning system are deterministic (i.e. they will always produce the same average $F_1$ values), so if this
experiment were run an arbitrary number of times the means of the distributions of average $F_1$ would be 0.9416 and
0.6913 respectively - the ANN system is clearly better suited to this particular task than the CBR system. Comparison
with the Decision Tree algorithm is slightly more difficult as it produces slighlty different results each time it is run.
However, by repeating this experiment 50 times and performing a t-test on the resulting vectors of average $F_1$ values,
it was discovered that the two distributions had different means (at the 95\% significance level), with the ANN mean being
higher. Although we cannot say that the ANN learning algorithm is the best learning algorithm in general, if it is assumed
that the clean and noisy datasets provided were representative of real data then we can at least say that the ANN algorithm
is best suited to the task of classifying noisy data when trained on clean data.\\

\newpage
\section{ANOVA Test Results (Clean Data)}

The results of ANOVA, multiple comparison and t-tests can be seen below:

\begin{center}
  \begin{tabular}{|c|c|c|c|}
  \hline
    {\bf Comparison} & {\bf ANOVA p-value} & {\bf Multiple Comparison [95\% c.i.]} & {\bf T-Test (H value)} \\
    \hline
    \multicolumn{4}{|c|}{Anger} \\ \hline
	DT vs. NN 	& 			& similar [-0.5518,0.1058] 	& 0 \\
	NN vs. CBR 	& 0.0526 	& similar [-0.4399,0.2177]	& 0 \\
	DT vs. CBR 	& 			& different [-0.6629,-0.0053] & 1 \\
	\hline
    \multicolumn{4}{|c|}{Disgust} \\ \hline
	DT vs. NN 	& 			& similar [-0.0585,0.0699]	& 0 \\
	NN vs. CBR 	& 0.9682 	& similar [-0.0642,0.0642]	& 0 \\
	DT vs. CBR 	& 			& similar [-0.0585,0.0699]	& 0 \\
	\hline
	\multicolumn{4}{|c|}{Fear} \\ \hline
	DT vs. NN 	& 			& similar [-0.5899,0.3219]	& 0 \\
	NN vs. CBR 	& 0.5648 	& similar [-0.5159,0.3959]  & 0 \\
	DT vs. CBR 	& 			& similar [-0.6499,0.2619]	& 0 \\
	\hline
	\multicolumn{4}{|c|}{Happiness} \\ \hline
	DT vs. NN 	& 			& similar [-0.1793,0.0057]	& 0 \\
	NN vs. CBR 	& 0.0408 	& similar [-0.0925,0.0925]	& 0 \\
	DT vs. CBR 	& 			& similar [-0.1793,0.0057]	& 0 \\
	\hline
	\multicolumn{4}{|c|}{Sadness} \\ \hline
	DT vs. NN 	& 			& similar [-0.4147,0.1483]	& 0 \\
	NN vs. CBR 	& 0.1014 	& similar [-0.4017,0.1613]	& 1 \\
	DT vs. CBR 	& 			& similar [-0.5349,0.0281]	& 0 \\
	\hline
	\multicolumn{4}{|c|}{Surprise} \\ \hline
	DT vs. NN 	& 			& similar [-0.3220,0.0820]	& 0 \\
	NN vs. CBR 	& 0.2531 	& similar [-0.2020,0.2020]	& 0 \\
	DT vs. CBR 	& 			& similar [-0.3220,0.0820]	& 0 \\
	\hline
  \end{tabular}\\
 \end{center}
 (DT = Decision Trees, NN = Neural Networks, CBR = Case-Based Reasoning)\\
 
 Little of interest can be gained from these results, as all algorithms performed well on clean data. A difference was found
 between classifications of `Anger' made by the Decision Tree algorithm and the Case-Based system by the multiple comparison
 test, explaining the low ANOVA value for that emotion, and the t-test between those two algorithms confirms this difference.
 In most other cases, both the ANOVA/Multiple Comparison tests and the t-tests were in agreement and no differences were found
 between the algorithms classification scores. Despite having a very low ANOVA score, no differences could be found by either
 of the follow-up tests between the average $F_1$ values for `Happiness'. The only case in which the tests produced different
 results was the classification of `Sadness' by Neural Networks and the Case-Based system, although this could be a type 1 error
 introduced by the number of t-tests performed - as tests were performed with an alpha value of 0.05, approximately one in every
 20 paired t-tests would incorrectly identify a difference, so one error could likely be expected when 18 tests were performed.\\
  
\newpage
\section{ANOVA Test Results (Noisy Data)}

The results of ANOVA, multiple comparison and t-tests can be seen below:

\begin{center}
  \begin{tabular}{|c|c|c|c|}
  \hline
    {\bf Comparison} & {\bf ANOVA p-value} & {\bf Multiple Comparison [95\% c.i.]} & {\bf T-Test (H value)} \\
    \hline
    \multicolumn{4}{|c|}{Anger} \\ \hline
	DT vs. NN 	& 			& similar [-0.3905,0.2171] 	& 0 \\
	NN vs. CBR 	& 0.7620 	& similar [-0.2371,0.3705]	& 0 \\
	DT vs. CBR 	& 			& similar [-0.3238,-0.2838] & 0 \\
	\hline
    \multicolumn{4}{|c|}{Disgust} \\ \hline
	DT vs. NN 	& 			& similar [0.0,0.0]	& 0 \\
	NN vs. CBR 	& -		 	& similar [0.0,0.0]	& 0 \\
	DT vs. CBR 	& 			& similar [0.0,0.0]	& 0 \\
	\hline
	\multicolumn{4}{|c|}{Fear} \\ \hline
	DT vs. NN 	& 			& similar [-0.4222,0.1840]	& 0 \\
	NN vs. CBR 	& 0.6420	& similar [-0.2556,0.3556]	& 0 \\
	DT vs. CBR 	& 			& similar [-0.3722,0.2390]	& 0 \\
	\hline
	\multicolumn{4}{|c|}{Happiness} \\ \hline
	DT vs. NN 	& 			& similar [-0.0256,0.0756]	& 0 \\
	NN vs. CBR 	& 0.3811 	& similar [-0.0250,0.0256]	& 0 \\
	DT vs. CBR 	& 			& similar [-0.0506,0.0506]	& 0 \\
	\hline
	\multicolumn{4}{|c|}{Sadness} \\ \hline
	DT vs. NN 	& 			& similar [-0.5256,0.1656]	& 0 \\
	NN vs. CBR 	& 0.4438 	& similar [-0.2656,0.4256]	& 0 \\
	DT vs. CBR 	& 			& similar [-0.1000,0.2456]	& 0 \\
	\hline
	\multicolumn{4}{|c|}{Surprise} \\ \hline
	DT vs. NN 	& 			& similar [-0.1287,0.0087]	& 0 \\
	NN vs. CBR 	& 0.0290 	& similar [-0.0830,0.0544]	& 0 \\
	DT vs. CBR 	& 			& different [-0.1430,-0.0056]	& 1 \\
	\hline
  \end{tabular}\\
 \end{center}
 (DT = Decision Trees, NN = Neural Networks, CBR = Case-Based Reasoning)\\
 
 For these results, all t-tests and multiple comparison tests agree in all cases. The only conclusion that can reliably be drawn from
 this analysis is that the Case-Based Reasoning system is very likely better at classifying surprise than the Decision Tree algorithm
 when trained and tested on noisy data. It should be noted that all three algorithms performed flawlessly when classifying Disgust,
 each achieving perfect scores for $F_1$ on every fold, so no further analysis was possible (or necessary).\\
 
\section{Additional Commentary}

% Provide comments as to why some of the examined algorithms performed better than the others.
{\bf Algorithm Performance}\\

As evidenced in the first section of this report, the `Neural Network' learning algorithm performed best when trained on clean examples
and subsequently tested on noisy data. This is perhaps explainable by the increased complexity and optimisation of the Neural Network
algorith used when compared with the others. For the Decision Tree and Case-Based reasoning tasks, a working implementation was the main goal
and any optimisation occurred after this was complete and the algorithms were fully tested for correctness. Conversely, the provided Neural
Network Toolbox drastically reduced the complexity of implementing the ANN system and allowed much more time for all parameters of the algorithm
to be optimised to produce better results. Another issue to consider is that of overfitting - likely to have caused problems for the systems when
they were trained with data which was noticable different from the data they would later be tested on. The Neural Network Toolbox provides tools
to mitigate the effects of overfitting, which are discussed in more detail in the second report, and which will have reduced the effects of
training with clean data and being tested on noisy data. Methods of reducing overfitting were investigated for the other algorithms (for example,
pruning for the Decision Trees) but these were not implemented in the final versions of these systems, meaning that their results may have
suffered because of overfitting to the training data.\\

% Suppose that we want to add some new classes of new emotions to the existing dataset. 
% Which of the examined algorithms are more suitable for incorporating the new classes 
% in terms of extra engineering effort? Which algorithms need to undergo radical changes 
% in order to include new classes?
{\bf Expandability}\\

The complexity of the Neural Network algorithm does, however, impact its expandability. If a new emotion were to be added to the list of
those to be classified, for example, the feeling of being surrounded by incompetence and apathy and being forced to complete a four-man project
alone, several aspects of the Neural Network algorithm would need to be altered to take account of the new problem domain. If the six single-output
networks were to be used, extra code would need to be added to generate this new network and new code would need to be added to take account of
this new emotion at the recombination stage when the most applicable label is chosen for an emotion (in `combine\_probability.m').
Even if the single 6-output network was used new code would be necessary to determine the most likely result of a query from a range of
probabilities returned (in the function `six\_output\_combine.m'). In addition, in both cases it is almost certain that the optimised parameters
found in task 2 would no longer produce the best results and these would have to be re-investigated. The Decision Tree algorithm would be
similarly affected by the addition of a new emotion as a tree is generated for each possible emotion and the results later recombined - a new tree
would be needed and the recombination code (in `testTrees.m' and `combine\_results.m') would require some changes. The Case-Based Reasoning system
lies at the opposite end of this complexity scale and would require almost no changes to adapt to a new emotion as it was designed to be expandable.
In fact, if an existing CBR system were retrained with examples which contained a new emotion label it would immediately be able to classify novel
examples with that label with oly one minor change to the codebase - a single {\bf for} loop in the method `CBR\_retrieve\_match\_pc.m' would need
to have it's number of iterations increased by one for each new emotion. It should be noted that certain helper functions (`remap\_labels.m' etc.)
and all analysis code (`confusion\_matrix.m' etc.) would need to be updated, but these are common to all three algorithms and are not required
for the algorithms to function, so they can be ignored when considering expandability.\\

% What assumptions do the t-test and ANOVA test make concerning the variances of the compared groups?
% Can you think of other procedures in case some of these assumptions do not hold?
{\bf Assumptions of ANVOA/T-Test}\\
 
Both the  ANOVA and the t-test assume that the variances of the populations being compared are homogeneous. If this is not he case, and two groups
(or more for ANOVA) have unequal variances, then the results of these tests cannot be relied upon and different procedures should be used
to compare the groups. In the case of the t-test, it is possible to use modified versions of the formulae from the test to find t-values
and their associated degrees of freedom which can then be used just like those calculated using the standard t-test.
This is known as `Welch's t-test', and it relies on the use of the sample variances in the calculation of the t statistic and the degrees of
freedom instead of the population variances, which are unknown. This will generate good results, but it should only be used if
it is known that the groups have significantly different variances and the sample size is small, as the standard t-test is usually more accurate
if this is not the case.
ANOVA can similarly be modified to produce better results when the population variances are unequal. By applying a tranformation to the data
before performing the ANOVA test (for example, taking logarithms of each value), it is sometimes possible to correct to inequality of variances
among populations. This can also be used to `normalise' data which do not originally fit a normal distribution, as normally distribution
of samples is another assumption of the ANOVA test. It should be noted, however, that this is usually only necessary if the sample sizes of the
various groups to be used are unequal or if there are significant differences in the variances of the compared groups. If the groups are not normally
distributed but do have equal variances, nonparametric tests such as the Kruskal-Wallis test can be used instead of ANOVA to compare them.
 
\newpage 
\section{Appendix}

$F_1$ results per fold from 10-fold cross-validation on clean data:\\

{\bf Decision Trees}\\

\begin{center}
  \begin{tabular}{|c|cccccccccc|}
  \hline
    Emotion & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
    \hline
    Anger 	& 0.333 & 0.8 & 0.667 & 0 & 0.667 & 1.0 & 1.0 & 1.0 & 0.0 & 1.0 \\
	Disgust & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 0.857 & 1.0 & 1.0 & 1.0 \\
	Fear 	& 0.667 & 1.0 & 0.0 & 0.0 & 1.0 & 1.0 & 1.0 & 0.0 & 1.0 & 1.0 \\
	Happiness 	& 0.667 & 1.0 & 0.667 & 0.8 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \\
	Sadness 	& 0.0 & 0.8 & 1.0 & 0.0 & 1.0 & 1.0 & 1.0 & 1.0 & 0.667 & 1.0 \\
	Surprise 	& 0.0 & 1.0 & 0.8 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \\
	\hline
  \end{tabular}\\
  \end{center}
  
 {\bf Neural Network}\\

\begin{center}
  \begin{tabular}{|c|cccccccccc|}
  \hline
    Emotion & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
    \hline
    Anger 	& 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 0.8 & 0.889 & 0.0 & 1.0 \\
	Disgust & 1.0 & 1.0 & 1.0 & 1.0 & 0.8 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \\
	Fear 	& 1.0 & 1.0 & 1.0 & 1.0 & 0.0 & 0.0 & 1.0 & 1.0 & 1.0 & 1.0 \\
	Happiness 	& 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \\
	Sadness 	& 1.0 & 1.0 & 1.0 & 1.0 & 0.8 & 0.667 & 0.667 & 0.667 & 1.0 & 1.0 \\
	Surprise 	& 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \\
	\hline
  \end{tabular}\\
  \end{center}
  
  {\bf Case-Based Reasoning}\\

\begin{center}
  \begin{tabular}{|c|cccccccccc|}
  \hline
    Emotion & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
    \hline
    Anger 	& 1.0 & 1.0 & 1.0 & 1.0 & 0.8 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \\
	Disgust & 0.8 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \\
	Fear 	& 0.667 & 1.0 & 1.0 & 1.0 & 0.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \\
	Happiness 	& 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \\
	Sadness 	& 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \\
	Surprise 	& 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \\
	\hline
  \end{tabular}\\
  \end{center}

$F_1$ results per fold from 10-fold cross-validation on noisy data:\\
  
{\bf Decision Trees}\\

\begin{center}
  \begin{tabular}{|c|cccccccccc|}
  \hline
    Emotion & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
    \hline
    Anger & 0.8 & 1.0 & 1.0 & 0.667 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 &0.0\\
Disgust   & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \\
Fear & 1.0 & 1.0 & 1.0 &0.0& 0.667 & 0.667 & 0.8 & 1.0 & 1.0 & 1.0 \\
Happiness & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \\
Sadness & 0.667 & 1.0 & 1.0 &0.0& 1.0 &0.0& 1.0 & 1.0 & 1.0 & 1.0 \\
Surprise & 1.0 & 1.0 & 1.0 & 0.857 & 0.8 & 1.0 & 0.8 & 1.0 & 1.0 & 0.8 \\
	\hline
  \end{tabular}\\
  \end{center} 
  
 {\bf Neural Network}\\

\begin{center}
  \begin{tabular}{|c|cccccccccc|}
  \hline
    Emotion & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
    \hline
    Ang & 1.0 & 1.0 & 0.667 & 1.0 & 1.0 & 0.667 & 1.0 & 1.0 & 1.0 & 1.0 \\
Disgust & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \\
Fear & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 0.8 & 0.5 & 1.0 \\
Happiness & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 0.75 & 1.0 \\
Sadness & 1.0 & 1.0 & 0.8 & 1.0 & 1.0 & 1.0 & 1.0 & 0.667 & 1.0 & 1.0 \\
Surprise & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 0.857 & 1.0 & 1.0 & 1.0 & 1.0 \\
	\hline
  \end{tabular}\\
  \end{center} 
  
 {\bf Case-Based Reasoning}\\

\begin{center}
  \begin{tabular}{|c|cccccccccc|}
  \hline
    Emotion & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
    \hline
    Ang & 1.0 & 1.0 & 1.0 & 0.667 &0.0& 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \\
Disgust & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \\
Fear 	&0.0& 1.0 & 1.0 & 1.0 & 0.8 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \\
Happiness & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \\
Sadness & 0.667 & 1.0 & 1.0 &0.0& 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \\
Surprise & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 \\
	\hline
  \end{tabular}\\
  \end{center} 
 
\end{document}