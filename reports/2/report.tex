\documentclass[a4paper]{article}
\usepackage{fullpage, titling, amsmath, footnote, listings, graphicx, subfig, lscape}
\makesavenoteenv{tabular}
\setlength{\droptitle}{-50pt}

\title{Machine Learning Assignment 3: Artificial Neural Networks}
\author{Group 31 \\ Chris Bates, Joe Slade, Andrew West, Thomas Wood}

\begin{document}
\maketitle
\section{Network Parameters}

%  What criteria have you used to choose optimal topology/parameters of
%  the networks? Compare the optimal parameters of both types of the networks
%  (the single six-output network and six single-outputs networks). Explain what
%  strategy you employed to ensure good generalisation ability of the networks
%  and overcome the problem of overfitting, if encountered (support this by
%  experimental results).

The first requirement of the task was to decide what constituted an `optimal' neural network. As the networks would eventually
be compared based on their performances when subjected to 10-fold cross-validation, it was decided that we should attempt to 
maximise the average $F_1$ value from between the six emotions when tested in this way. Thus, the `optimal' networks would be those that coped
best when tested on unseen data, in a similar manner to the way success of the decision trees of the previous task were measured.\\

Following feedback from the previous task, the function used to subdivide the data set for 10-fold cross-validation was improved
to group data into test and training sets randomly in order to make our results less dependent on the arrangement of the underlying data.
Thus, a secondary concern when searching for the optimal solution was that the networks be trainable in a reasonable amount of time.
This ensured that it was possible to collect large amounts of test data for each variation of each network parameter in the
limited time available, which could then be used to counteract the effects of randomness on our results and ensure our findings
were statistically significant and supported by our data.\\

The actual optimisation of the parameters was effectively done by brute force. By using Condor to perform large numbers of computations
beyond the scope of what would have been achievable using individual machines, it was possible to test large numbers of different values
for each parameter. These experiments were also repeated many times to ensure their accuracy. In each experiment, one parameter was varied
while the others remained fixed. This allowed us to see that impact each had on the performance of the resulting neural networks. In ideal
circumstances, one extremely large experiment in which every permutation of possible parameter combinations would have instead been run,
but because of time constraints on the task it was decided to take a more simplistic approach to optimising the network parameter.\\

The first parameters to be tested were the number of hidden layers and number of neurons in each layer. Neural networks were trained
with a single hidden layer containing from 1 to 50 neurons, and then with two hidden layers each of which contained up to 50 neurons.
A small number of tests with three hidden layers were attempted, but initial results showed no improvement over those with two layers. The
results of these can be seen in figures 1 and 2. Each plots the number of neurons in each hidden layer against the average $F_1$ achieved by
networks using that topology. Unfortunately, neither showed any particular trends or patterns which could be used to clearly identify the
optimum network topography. By using the graphs and the data returned by Condor we determined the optimum number of hidden layers to be
two for both types of network. The optimum number of neurons in the single six-output network was found to be 34 in the first layer and 15 in the
second, while the optimum values for the single output networks were 48 in the first layer and 14 in the second. However, networks of these sizes
would have taken a huge amount of time to train and run for the purposes of other tests, so smaller networks with average $F_1$ values 
within 0.05 of these maxima were sought. Hence, our optimised topologies were fixed at [10,5] for the single network and [8,4] 
for each of the six single-output networks.\\

Beyond some simple observations, little can be said about the effect of topography on these networks as the generally performed well regardless of 
the number of neurons per layer,
with very poor performances only observed from both networks when four or fewer neurons were present in their first layers. The six-output network
does appear to be more succeptable to changes in network topography however, as the range of $F_1$ values recorded by the experiment was far larger
than that of the single-output networks, going as low as 0.2 in some cases, while the lowest $F_1$ achived by the single-output networks was around 0.6.
Both achieved fairly high maximum values of 0.9336 and 0.9312 respectively.\\

The next parameters tested were the various transfer functions used between layers of the networks. Although the task specification identifies
three specifically to test for - perceptron (hardlim), linear unit (purelin) and sigmoid (tansig) - it was decided that Condor could be used
to test all 14 of the available transfer functions in the Matlab Neural Netorks Toolbox. The average $F_1$ values obtained after performing
10-fold cross-validation on the generated networks with differing transfer functons were again plotted, the results of which can be seen in figure 3.
Here, very clear distinctions can be seen between the different types of function, with the step functions (compet, hardlim and hardlims) all
performing very poorly on both networks, perhaps due to the restricted nature of the results they produce. The continuous functions have mostly
produced good results on both types of network, but it appears that linear functions are slightly better than sigmoid ones, and the best transfer
function for both types of network is pure linear (purelin).\\

Finally, the training functions and learning rates (where applicable) were tested. There are a large number of possible training functions
provided in the Toolbox, and in order to reduce the number of tests which would need to be run it was decided that functions which took longer
than the default 100 epochs to produce good results would be ignored. This had the additional benefit of ensuring all the test were quick to run,
which was one of the original `optimum' requirements. 17 functions were tested, some of which used learning rates, and
the results were again plotted in a similar manner to the previous two experiments in figure 4. For this initial test, learning rates were kept
constant for those functions which used them to make comparison easier. It can be seen from the graph that many of the more complex functions
did not perform as expected, with most failing to outperform the default function (trainlm). However, one function can be clearly identified
as the best from this test - the `trainbfg' function. Once this benchmark was established, all functions which required learning rates were then
tested separately using various lr values, and the results of these were compared with the results yielded by the trainbfg function.
It was found that none of these could be made to perform as well as trainbfg for any tested learning rate, and trainbfg was therefore selected
as the optimum function.\\ 

Two precautions were taken to ensure that all results obtained were correct and did not represent overfitted networks. Firstly, all networks were
optimised using 10-fold cross-validation and not on the full data set, meaning that networks which were overtrained would perform badly when tested
and would therefore be discarded. Secondly, the Matlab Neural Network toolbox provides a method to check for overfitting while training networks and
this was also used to check all generated networks would still function well when introduced to new data. Matlab neural networks have a `divideFcn'
parameter which can be used to divide all training data into training, validation and testing sets. As 10-fold cross-validation was already being
used, it was not necessary for the divideFcn to create a testing set from the given data, but by assigning 20\% of all training examples to the
validation set the Neural Network Toolbox could automatically detect and adjust for overfitting.\\ 

\newpage

%  perform 10-fold cross-validation for both types of networks. Cross-validation
%  should be performed in the same way as in Assignment 2 (with a script that
%  splits the given dataset into training and test sets). 10-fold cross-validation
%  should be performed using the optimal topology and best parameters obtained
%  in 2(a), i.e., for six-output and single-output NNs. Note that in the case of 6
%  networks, each example must be classified as one of the 6 emotions. Plot the
%  performance (only F1 measure) per fold of each network type in the same
%  figure.

\section{Results of Testing}


A plot of the average $F_1$ measure for each fold of a 10-fold cross-validation for both networks with optimal parameters can be seen below:
\begin{center}
\includegraphics[height=70mm]{10-fold-plot.png}
\end{center}
\begin{flushleft}
Both networks have similar success rates, often achieving perfect results ($F_1$ value of 1). These perfect results often occurred on the same
fold for both networks, specifically on folds 2,3,7 and 8. This suggests that there is little difference between the two types of network when
trained on the same data. Confusion matricies and precision/recall table for both types of network from a different run of the 
10-fold cross-validation script can be seen below:\\
\end{flushleft}

%  Report also the average results of the 10-fold cross-validation (similarly to the
%  trees) for both types of networks (single six-output NN and six single-outputs
%  NNs):
%     confusion matrices,
%     recall and precision rates per class,
%       (Hint: you can derive them directly from the previously computed  confusion matrix)
%     the F1-measure derived from the recall and precision rates of the previous step.

{\bf Single Six-Output Network Results}

 \begin{center}
	
	Confusion Matrix\\
	
  \begin{tabular}{c|cccccc}
    & Anger & Disgust & Fear & Happiness & Sadness & Surprise \\
    \hline
    Anger & 11 & 0 & 1 & 0 & 0 & 0 \\
  Disgust & 0 & 21 & 1 & 0 & 0 & 0 \\
     Fear & 1 & 0 & 6 & 0 & 0 & 0 \\
Happiness & 0 & 0 & 0 & 24 & 0 & 0 \\
  Sadness & 0 & 0 & 0 & 0 & 11 & 1 \\
 Surprise & 0 & 0 & 0 & 0 & 0 & 23 \\
  \end{tabular}\\
  \end{center}
  
   \begin{center}
  
  Precision/Recall Table\\
  
  \begin{tabular}{c|cccc|ccc}
    Emotion & TP & TN & FP & FN & Recall & Precision & $F_1$ \\
    \hline
    Anger & 11 & 87 & 1 & 1 & 0.917	& 0.917	& 0.917 \\
  Disgust & 21 & 78 & 0 & 1 & 1.000	& 0.955	& 0.977 \\
     Fear & 6 & 91 & 2 & 1 & 0.750	& 0.857	& 0.800 \\
Happiness & 24 & 76 & 0 & 0 & 1.000 & 1.000 & 1.000 \\
  Sadness & 11 & 88 & 0 & 1 & 1.000	& 0.917	& 0.957 \\
 Surprise & 23 & 76 & 1 & 0 & 0.958	& 1.000	& 0.979 \\
  \end{tabular}\\
  
 \end{center}
  
{\bf Six Single-Output Networks Results}

 \begin{center}

	Confusion Matrix\\

	\begin{tabular}{c|cccccc}
    & Anger & Disgust & Fear & Happiness & Sadness & Surprise \\
    \hline
    Anger & 10 & 0 & 0 & 0 & 2 & 0 \\
  Disgust & 0 & 21 & 0 & 0 & 1 & 0 \\
     Fear & 0 & 0 & 7 & 0 & 0 & 0 \\
Happiness & 0 & 0 & 0 & 24 & 0 & 0 \\
  Sadness & 3 & 0 & 0 & 0 & 9 & 0 \\
 Surprise & 0 & 0 & 0 & 0 & 0 & 23 \\
  \end{tabular}\\
  \end{center}
  
  \begin{center}
  
  Precision/Recall Table\\
  
  \begin{tabular}{c|cccc|ccc}
    Emotion & TP & TN & FP & FN & Recall & Precision & $F_1$ \\
    \hline
    Anger & 10 & 85 & 3 & 2 & 0.769	& 0.833	& 0.800 \\
  Disgust & 21 & 78 & 0 & 1 & 1.000	& 0.955	& 0.977 \\
     Fear & 7 & 93 & 0 & 0 & 1.000	& 1.000	& 1.000\\
Happiness & 24 & 76 & 0 & 0 & 1.000 & 1.000 & 1.000 \\
  Sadness & 9 & 85 & 3 & 3 & 0.750	& 0.750	& 0.750\\
 Surprise & 23 & 77 & 0 & 0 & 1.000	& 1.000	& 1.000 \\
  \end{tabular}\\
  \end{center}


%  Is there any difference in the classification performance of the two different
%  classification approaches. Discuss the advantages / disadvantages of using 6
%  single-output NNs vs. 1 six-output NNs.

In general, results were very similar for both types of network, with the single six-output network marginally outperforming the six single-output
networks. The results above do demonstrate a trend that was noticed throughout testing, where the single output networks generated for emotions with
large numbers of samples in the training set performed very well, while those that were created for emotion with fewer samples (anger and sadness 
above) often not recognised. This is likely due to the limited scope of the networks in training and the likelyhood that during 10-fold cross-validation
some training sets contained very few samples of a given emotion. Interstingly, this did not happen as frequently for fear despite its low number
of examples. However, this may just indicate that the optimum training parameters found were particularly effective at training networks to detect
fear due to some charicteristic of the specific AUs fear activates. The single six-output network was more resistant to errors made due to a low
number of examples in the training data, showing a generally more evenly distributed pattern of mis-classifications as would be expected from a
more generalised network.

The main advantage of the single network when compared to the six single output networks is the speed of training. Each of the six single-output 
networks takes a similar amount of time to train as the single six output one, leading to large training times for some of the more complex training
functions, including the one eventually chosen as the optimum. However, for recognizing a single emotion or a small subset of the basic emotions
the six single-output trees do perform better due to their increased specialization, as long as adequate precautions against overfitting are taken. 

\begin{figure}[p]
  \centering
  {\includegraphics[height=90mm]{6-out-topology.png}}
  \caption{Isometric and top-down view of surface created by plotting neurons per layer \\ against average $F_1$ value for six-output network}
\end{figure}

\begin{figure}[p]
  \centering
  {\includegraphics[height=90mm]{1-out-topology.png}}
  \caption{Isometric and top-down view of surface created by plotting neurons per layer \\ against average $F_1$ value for single-output networks}
\end{figure}

\begin{landscape}
\begin{figure}[p]
  \centering
  {\includegraphics[height=120mm]{transfer-function.png}}
  \caption{Results of varying transfer function used}
\end{figure}

\begin{figure}[p]
  \centering
  {\includegraphics[height=120mm]{training-function.png}}
  \caption{Results of varying training function used (constant learning rate)}
\end{figure}
\end{landscape}

\end{document}